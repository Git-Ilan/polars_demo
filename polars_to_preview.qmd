---
title: "Intro 2 Polars"
execute:
  warning: true
  error: true
  keep-ipynb: true
  cache: true
jupyter: python3
html:
    code-tools: true
    fold-code: false
    author: Jonathan D. Rosenblatt
    data: 02-01-2023
    toc: true
    number-sections: true
    embed-resources: true
---

```{python}
import polars as pl
import pandas as pd
import numpy as np
import pyarrow as pa
import plotly.express as px
import string
import random
import os
%matplotlib inline 
import matplotlib.pyplot as plt
from datetime import datetime

# Following two lines only required to view plotly when rendering from VScode. 
import plotly.io as pio
pio.renderers.default = "plotly_mimetype+notebook_connected"

```



# Motivation
Each of the following, alone(!), is amazing.

1. Small memory footpring
    - Native dtypes: missing, strings.
    - Arrow format in memory.
1. Lazy evaluation allows query Planning.
1. Out of the box parallelism: Fast and informative mesages for debugging.
1. Strict typing: This means the dtype of output is defined by the operation and not bu the input. This is both safers, and allows static analysis.



## Memory Footprint


### Memory Footprint of Storage

Polars vs. Pandas:
```{python}
letters = pl.Series(list(string.ascii_letters))

n = int(10e6)
letter1 = letters.sample(n,with_replacement=True)
letter1.estimated_size(unit='gb')
```


```{python}
letter1_pandas = letter1.to_pandas() 
letter1_pandas.memory_usage(deep=True, index=False) / 1e9
```

The memory footprint of the polars Series is 1/7 of the pandas Series(!).
But I did cheat- I used string type data to emphasize the difference. The difference would have been smaller if I had used integers or floats. 




### Memory Footprint of Compute

You are probably storing your data to compute with it.
Let's compare the memory footprint of computations. 

```{python}
%load_ext memory_profiler
```


```{python}
%memit letter1.sort()
```

```{python}
%memit letter1_pandas.sort_values()
```


```{python}
%memit letter1[10]='a'
```

```{python}
%memit letter1_pandas[10]='a'
```

Things to notice:

- Operating on existing data consumes less memory in polars than in pandas.
- Changing the data consumes more memory in polars than in pandas. Why is that?


### Operating From Disk to Disk

What if my data does not fit into RAM?
Turns out you can read from disk, process in RAM, and write to disk. This allows you to process data larger than your memory. 

TODO: demonstrate sink_parquet from [here](https://www.rhosignal.com/posts/sink-parquet-files/).





## Query Planning

Consider a sort opperation that follows a filter operation. 
Ideally, filter precededs the sort, but we did not ensure this...
We now demonstarte that polars' query planner will do it for you. 
En passant, we see polars is more efficient also without the query planner. 


Polars' Eager evaluation, without query planning. 
Sort then filter. 
```{python}
%timeit -n 2 -r 2 letter1.sort().filter(letter1.is_in(['a','b','c']))
```

Polars' Eager evaluation, without query planning. 
Filter then sort. 
```{python}
%timeit -n 2 -r 2 letter1.filter(letter1.is_in(['a','b','c'])).sort()
```


Polars' Lazy evaluation with query planning. 
Recieves sort then filter; executes filter then sort. 
```{python}
%timeit -n 2 -r 2 letter1.alias('letters').to_frame().lazy().sort(by='letters').filter(pl.col('letters').is_in(['a','b','c'])).collect()
```


Pandas' eager evaluation in the wrong order: Sort then filter. 
 ```{python}
%timeit -n 2 -r 2 letter1_pandas.sort_values().loc[lambda x: x.isin(['a','b','c'])]
```

Pandas eager evaluation in the right order: Filter then sort. 
```{python}
%timeit -n 2 letter1_pandas.loc[lambda x: x.isin(['a','b','c'])].sort_values()
```

Pandas alternative syntax, just as slow. 
```{python}
%timeit -n 2 -r 2 letter1_pandas.loc[letter1_pandas.isin(['a','b','c'])].sort_values()
```


Things to note:

1. Query planning works!
1. Polars faster than Pandas even in eager evaluation (without query planning).



## Parallelism

Polars seamlessly parallelizes over columns (also within, when possible).
As the number of columns in the data grows, we would expect fixed runtime until all cores are used, and then linear scaling.
The following code demonstrates this idea, using a simple sum-within-column.

```{python}
import time

def scaling_of_sums(n_rows, n_cols):
  # n_cols = 2
  # n_rows = int(1e6)
  A = {}
  A_numpy = np.random.randn(n_rows,n_cols)
  A['numpy'] = A_numpy.copy()
  A['polars'] = pl.DataFrame(A_numpy)
  A['pandas'] = pd.DataFrame(A_numpy)

  times = {}
  for key,value in A.items():
    start = time.time()
    value.sum()
    end = time.time()
    times[key] = end-start

  return(times)
```


```{python}
scaling_of_time = {
  p:scaling_of_sums(n_rows= int(1e6),n_cols = p) for p in np.arange(1,16)}
```


```{python}
data = pd.DataFrame(scaling_of_time).T
fig = px.line(
  data, 
  labels=dict(
    index="Number of Columns", 
    value="Runtime")
)
fig.show()
```


Things to note:

- Pandas is slow. 
- Numpy is quite efficient.
- My machine has 8 cores. I would thus expect a fixed timing until 8 columns, and then linear scaling. This is not the case. I wonder why?


## Speed Of Import

Polar's `read_x` functions are quite faster than Pandas. 
This is due to better type "guessing" heuristics, and to native support of the parquet file format. 

We now make synthetic data, save it as csv or parquet, and reimport it with polars and pandas.

Starting with CSV:
```{python}
n_rows = int(1e5)
n_cols = 10
data = np.random.randn(n_rows,n_cols)
data.tofile('data/data.csv', sep = ',')
```

Import with pandas. 
```{python}
%timeit -n2 -r2 data_pandas = pd.read_csv('data/data.csv', header = None)
```

Import with polars. 
```{python}
%timeit -n2 -r2 data_polars = pl.read_csv('data/data.csv', has_header = False)
```



Moving to parquet:

```{python}
data_pandas = pd.DataFrame(data)
data_pandas.columns = data_pandas.columns.astype(str)
data_pandas.to_parquet('data/data.parquet', index = False)
```

```{python}
%timeit -n2 -r2 data_pandas = pd.read_parquet('data/data.parquet')
```

```{python}
%timeit -n2 -r2 data_polars = pl.read_parquet('data/data.parquet')
```



TODO: add feather.




Things to note:

- The difference in speed is quite large.
- I dare argue that polars' type guessing is better, but I am not demonstrating it here. 
- Bonus fact: parquet is much faster than csv, and also saves the frame's schema.



## Speed Of Join

Because pandas is built on numpy, people see it as both an in-memory database, and a matrix/array library.
With polars, it is quite clear it is an in-memory database, and not an array processing library (despite having a `pl.dot()` function for inner products).
As such, you cannot multiply two polars dataframes, but you can certainly join then efficiently.

Make some data:
```{python}
def make_data(n_rows, n_cols):
  data = np.concatenate(
  (
    np.arange(n_rows)[:,np.newaxis], # index
    np.random.randn(n_rows,n_cols), # values
    ),
    axis=1)
    
  return data


n_rows = int(1e6)
n_cols = 10
data_left = make_data(n_rows, n_cols)
data_right = make_data(n_rows, n_cols)
```

Polars join:
```{python}
data_left_polars = pl.DataFrame(data_left)
data_right_polars = pl.DataFrame(data_right)

%timeit -n2 -r2 polars_joined = data_left_polars.join(data_right_polars, on = 'column_0', how = 'inner')
```

Pandas join:
```{python}
data_left_pandas = pd.DataFrame(data_left)
data_right_pandas = pd.DataFrame(data_right)

%timeit -n2 -r2 pandas_joined = data_left_pandas.merge(data_right_pandas, on = 0, how = 'inner')
```



## Moving Forward...

If this motivational seection has convinced you to try polars instead of pandas, here is a  more structured intro. 






# Polars Series

Much like pandas, polars' fundamental building block is the series. 
A series is a column of data, with a name, and a dtype.
In the following we:

1. Create a series and demonstrate basic operations on it.
1. Demonstrate the various dtypes. 
1. Discuss missing values.
1. Filter a series.

## Series Housekeeping
Construct a series
```{python}
s = pl.Series("a", [1, 2, 3])
s
```

Make pandas series for comparison:
```{python}
s_pandas = pd.Series([1, 2, 3], name = "a")
```


```{python}
type(s)
```

```{python}
type(s_pandas)
```


```{python}
s.dtype
```

```{python}
s_pandas.dtype
```

Renaming a series; will be very useful when operating on dataframe columns.
```{python}
s.alias("b")
```

```{python}
s.clone()
```

```{python}
s.clone().append(pl.Series("a", [4, 5, 6]))
```
Note: `series.append` operates in-place. That is why we cloned the series first.

Flatten a list of lists using `explode()`.
```{python}
pl.Series("a", [[1, 2], [3, 4], [9, 10]]).explode()
```


```{python}
s.extend_constant(666, n=2)
```

```{python}
#| eval: false
s.new_from_index()
```

```{python}
s.rechunk()
```
```{python}
s.rename("b", in_place=False) # has an in_place option. Unlike .alias()
```

```{python}
s.to_dummies()
```



```{python}
s.cleared() # creates an empty series, with same dtype
```


Consturcting a series of floats, for later use.
```{python}
f = pl.Series("a", [1., 2., 3.])
f
```


```{python}
f.dtype
```





## Memory Representation of Series

Object size in memory. Super useful for profiling:
```{python}
s.estimated_size(unit="gb")
```



```{python}
s.chunk_lengths() # what is the length of each memory chunk?
```






## Filtering and Subsetting

```{python}
s[0]
```

Filtering with boolneas requires a series of booleans, not a list:
```{python}
s.filter(pl.Series("a", [True, False, True])) # works
```

Will not work:
```{python}
#| eval: true

s[[True, False, True]]
```

Don't be confused with pandas!
```{python}
#| eval: false

s.loc[[True, False, True]] 
```

```{python}
s.head(2)
```
```{python}
s.limit(2)
```

Negative indexing is not supported [TODO: verify]:
```{python}
#| eval: false

s.head(-1)
s.limit(-1)
```
```{python}
s.tail(2)
```
```{python}
s.sample(2, with_replacement=False)
```

```{python}
s.take([0, 2]) # same as .iloc
```

```{python}
s.slice(1, 2) # same as pandas .iloc[1:2]
```

```{python}
s.take_every(2)
```




## Aggregations
```{python}
s.sum()
```
```{python}
s.min()
```
```{python}
s.arg_min()
```
```{python}
s.mean()
```
```{python}
s.median()
```
```{python}
s.entropy()
```

```{python}
s.describe()
```

```{python}
s.value_counts()
```



## Object Transformations

```{python}
pl.Series("a",[1,2,3,4]).reshape(dims = (2,2))
```
```{python}
s.shift(1)
```

```{python}
s.shift(-1)
```

```{python}
s.shift_and_fill(1, 999)
```




## Mathematical Transformations
```{python}
s.abs()
```
```{python}
s.sin()
```

```{python}
s.exp()
```
```{python}
s.hash()
```
```{python}
s.log()
```

```{python}
s.peak_max()
```

```{python}
s.sqrt()
```

```{python}
s.clip_max(2)
```
```{python}
s.clip_min(1)
```


You cannot round integers, but you can round floats.

```{python}
f.round(2)
```
```{python}
f.ceil()
```
```{python}
f.floor()
```

```{python}
s.is_in(pl.Series([1, 10]))
```

__Caution__: `is_in()` in polars has an underscore, unlike `isin()` in pandas.



## Apply

Applying your own function:
```{python}
s.apply(lambda x: x + 1)
```

Using your own functions comes with a performance cost:
```{python}
s1 = pl.Series(np.random.randn(int(1e5)))

%timeit -n2 -r2 s1+1
```
```{python}
%timeit -n2 -r2 s1.apply(lambda x: x + 1)
```



## Cummulative Operations

```{python}
s.cummax()
```
```{python}
s.cumsum()
```
```{python}
s.cumprod()
```

```{python}
s.ewm_mean(com=0.5)
```



## Sequential Operations

```{python}
s.diff()
```

```{python}
s.pct_change()
```


## Windowed Operations

```{python}
s.rolling_apply(
  pl.sum, 
  window_size=2)
```


Not all functions will work within a `rolling_apply`! Only polars' functions will.
```{python}
#| eval: false

s.rolling_apply(np.sum, window_size=2) # will not work
```

```{python}
s.rolling_max(window_size=2)
```



```{python}
s.clip(1, 2)
```
```{python}
s.clone()
```
```{python}
# check equality with clone
s == s.clone()
```

## Booleans

```{python}
b = pl.Series("a", [True, True, False])
b.dtype
```
```{python}
b.all()
```
```{python}
b.any()
```








## Uniques and Duplicates

```{python}
s.is_duplicated()
```
```{python}
s.is_unique()
```
```{python}
s.n_unique()
```
```{python}
pl.Series([1,2,3,4,1]).unique_counts()
```

The first appearance of a value in a series:
```{python}
pl.Series([1,2,3,4,1]).is_first()
```




## dtypes

__Note__. Unlike pandas, polars' test functions have an underscore: `is_numeric()` instead of `isnumeric()`.


### Testing
```{python}
s.is_numeric()
```
```{python}
s.is_float()
```
```{python}
s.is_utf8()
```

```{python}
s.is_boolean()
```
```{python}
s.is_datelike()
```

Compare with Pandas Type Checkers:
```{python}
pd.api.types.is_string_dtype(s_pandas)
```

```{python}
pd.api.types.is_string_dtype(s)
```


### Casting

```{python}
s.cast(pl.Int32)
```

Things to note: 

- `s.cast()` is an in place operation. If you want to keep the original series, you can use `s.cast(pl.Int32).clone()`.
- `cast()` is polars' equivalent of pandas' `astype()`.
- For a list of dtypes see the official [documentation](see https://pola-rs.github.io/polars/py-polars/html/reference/datatypes.html).



### Optimizing dtypes

Find the most efficient dtype for a series:
```{python}
s.shrink_dtype()
```
Also see [here](http://braaannigan.github.io/software/2022/10/31/polars-dtype-diet.html).

Shrink the memory allocation to the size of the actual data (in place).
```{python}
s.shrink_to_fit() 
```




## Ordering and Sorting 

```{python}
s.sort()
```

```{python}
s.reverse()
```
```{python}
s.rank()
```
```{python}
s.arg_sort() 
```
`arg_sort()` returns the indices that would sort the series. Same as R's `order()`.

```{python}
s.sort() == s[s.arg_sort()]
```

`arg_sort()` can also be used to return the original series from the sorted one:
```{python}
s == s[s[s.arg_sort()].arg_sort()]
```

```{python}
s.shuffle(seed=1)
```


## Missing

Pandas users will be excited to know that polars has built in missing value support (!) for all dtypes.
This has been a long awaited feature in the Python data science ecosystem, with implications on performance and syntax.

```{python}
m = pl.Series("a", [1, 2, None, np.nan])
m.is_null()
```

```{python}
m.is_nan()
```

```{python}
m1 = pl.Series("a", [1, None, 2, ]) # python native None
m2 = pl.Series("a", [1, np.nan, 2, ]) # numpy's nan
m3 = pl.Series("a", [1, float('nan'), 2, ]) # python's nan
m4 = pd.Series([1, None, 2 ])
m5 = pd.Series([1, np.nan, 2, ])
m6 = pd.Series([1, float('nan'), 2, ])
```

```{python}
[m1.sum(), m2.sum(), m3.sum(), m4.sum(), m5.sum(), m6.sum()]
```


Things to note:

- The use of `is_null()` instead of pandas `isna()`.
- Polars supports `np.nan` but that is a different dtype than `None` (which is a `Null` type). `None` is not considered 
- Aggregating pandas and polars series behave differently w.r.t. missing values:
  - Both will ignore `None`; which is unsafe.
  - Polars will not ignore `np.nan`; which is safe. Pandas is unsafe w.r.t. `np.nan`, and will ignore it. 


Filling missing values; `None` and `np.nan` are treated differently:
```{python}
m1.fill_null(0)
```

```{python}
m1.interpolate()
```

```{python}
m2.fill_null(0)
```

```{python}
m2.fill_nan(0)
```

```{python}
m1.drop_nulls()
```

```{python}
m1.drop_nans()
```


```{python}
m2.drop_nulls()
```


## Export

```{python}
s.to_frame()
```
```{python}
s.to_list()
```
```{python}
s.to_numpy()
```
```{python}
s.to_pandas()
```
```{python}
s.to_arrow()
```


## Strings 
Like Pandas, accessed with the `.str` attribute.

```{python}
st = pl.Series("a", ["foo", "bar", "baz"])
```

```{python}
st.str.n_chars() # gets number of chars. In ASCII this is the same as lengths()
```

```{python}
st.str.lengths() # gets number of bytes in memory
```


```{python}
st.str.concat("-")
```

```{python}
st.str.contains("foo|tra|bar")
```


```{python}
st.str.count_match(pattern= 'o') # count literal metches
```

Count pattern matches. 
Notice the `r"<regex pattern>"` syntax for regex (more about it [here](https://stackoverflow.com/questions/2241600/python-regex-r-prefix)). 
```{python}
st.str.count_match(r"\w") # regex for alphanumeric
```


```{python}
st.str.ends_with("oo")
```

```{python}
st.str.starts_with("fo")
```


To extract the first appearance of a pattern, use `extract`:
```{python}
url = pl.Series("a", [
            "http://vote.com/ballon_dor?candidate=messi&ref=polars",

            "http://vote.com/ballon_dor?candidate=jorginho&ref=polars",

            "http://vote.com/ballon_dor?candidate=ronaldo&ref=polars"
            ])

url.str.extract(r"=(\w+)", 1)
```

To extract all appearances of a pattern, use `extract_all`:
```{python}
url.str.extract_all("=(\w+)")
```


```{python}
st.str.ljust(8, "*")
```

```{python}
st.str.rjust(8, "*")
```


```{python}
st.str.lstrip('f')
```


```{python}
st.str.rstrip('r')
```

Replacing first appearance of a pattern:
```{python}
st.str.replace(r"o", "ZZ")  
```

```{python}
st.str.replace(r"o+", "ZZ")  
```

Replace all appearances of a pattern:
```{python}
st.str.replace_all("o", "ZZ")
```

String to list of strings. Number of spits inferred.
```{python}
st.str.split(by="o")
```

```{python}
st.str.split(by="a", inclusive=True)
```

String to dict of strings. Number of splits fixed.
```{python}
st.str.split_exact("a", 2)
```

String to dict of strings. Length of output fixed.
```{python}
st.str.splitn("a", 4)
```

Strip white spaces.
```{python}
st.str.rjust(8, " ").str.strip()
```



```{python}
st.str.to_uppercase()
```

```{python}
st.str.to_lowercase()
```

```{python}
st.str.zfill(5)
```

```{python}
st.str.slice(offset=0, length=2)
```





## Date and Time

There are 4 datetime dtypes in polars:

1. Date: A date, without hours. Generated with `pl.Date()`.
2. Datetime: Date and hours. Generated with `pl.Datetime()`.
3. Duration: As the name suggests. Similar t o `timedelta` in pandas. Generated with `pl.Duration()`.
4. Time: Hour of day. Generated with `pl.Time()`.


### Converting from Strings

```{python}
sd = pl.Series(
    "date",
    [
        "2021-04-22",
        "2022-01-04 00:00:00",
        "01/31/22",
        "Sun Jul  8 00:34:60 2001",
    ],
)
sd.str.strptime(pl.Date, "%F", strict=False)
```

```{python}
sd.str.strptime(pl.Date, "%F %T",strict=False)
```

```{python}
sd.str.strptime(pl.Date, "%D", strict=False)
```



### Time Range

```{python}
from datetime import datetime, timedelta

start = datetime(year= 2001, month=2, day=2)
stop = datetime(year=2001, month=2, day=3)

date = pl.date_range(
  low=start, 
  high=stop, 
  interval=timedelta(seconds=500*61))
date
```

Things to note:

- How else could I have constructed this series? What other types are accepted as `low` and `high`? 
- `pl.date_range` may return a series of dtype `Date` or `Datetime`. This depens of the granularity of the inputs. 


```{python}
date.dtype
```

Cast to different time unit. 
May be useful when joining datasets, and the time unit is different.
```{python}
date.dt.cast_time_unit(tu="ms")
```



### From Date to String

```{python}
date.dt.strftime("%Y-%m-%d")
```




### Ecxtract Time Sub-Units

```{python}
date.dt.second()
```

```{python}
date.dt.minute()
```

```{python}
date.dt.hour()
```

```{python}
date.dt.day()
```


```{python}
date.dt.week()
```

```{python}
date.dt.weekday()
```

```{python}
date.dt.month()
```

```{python}
date.dt.year()
```

```{python}
date.dt.ordinal_day() # day in year
```

```{python}
date.dt.quarter()
```



### Durations 

Equivalent to Pandas `period` dtype.

```{python}
diffs = date.diff()
diffs
```

```{python}
diffs.dtype
```

```{python}
diffs.dt.seconds()
```

```{python}
diffs.dt.minutes()
```

```{python}
diffs.dt.days()
```

```{python}
diffs.dt.hours()
```


### Date Aggregations
Note that aggregating dates, returns a `datetime` type object. 

```{python}
date.dt.max()
```

```{python}
date.dt.min()
```

I have no idea what is an "average date", but it can be computed.
```{python}
date.dt.mean()
```

```{python}
date.dt.median()
```


### Data Transformations

Notice the syntax of `offset_by`. It is similar to R's `lubridate` package.
```{python}
date.dt.offset_by(by="1y2m20d")
```

Nagative offset is also allowed.
```{python}
date.dt.offset_by(by="-1y2m20d")
```

```{python}
date.dt.round("1y")
```

```{python}
date2 = date.dt.truncate("30m") # round to period
pd.crosstab(date,date2)
```




## Comparing Series 
```{python}
s.series_equal(pl.Series("a", [1, 2, 3]))
```







# DataFrames

General:

1. There is no row index (like R's `data.frame`, `data.table`, and `tibble`; unlike Python's `pandas`). 
1. Will not accept duplicat column names (unlike pandas).


## DataFrame-Object Hosekeeping

A frame can be created as you would expect. 
From a dictionary of series, a numpy array, a pandas sdataframe, or a list of polars (or pandas) series, etc.

```{python}
df = pl.DataFrame({
  "integer": [1, 2, 3], 
  "date": [
    (datetime(2022, 1, 1)), 
    (datetime(2022, 1, 2)), 
    (datetime(2022, 1, 3))], 
    "float":[4.0, 5.0, 6.0],
    "string": ["a", "b", "c"]})

df
```

```{python}
print(df)
```

Things to note:

1. The frame may be printed with Jupter's styling, or as ASCII with a `print()` statement.
1. Shape, and dtypes, are part of the output.

```{python}
df.columns
```

```{python}
df.shape
```

```{python}
df.height # probably more useful than df.shape[0]
```

```{python}
df.width
```


```{python}
df.schema # similar to pandas info()
```

```{python}
df.with_row_count()
```

Add a single column
```{python}
df.with_column(pl.Series("new", [1, 2, 3]))
```

Add multiple columns
```{python}
df.with_columns([
  pl.Series("new1", [1, 2, 3]),
  pl.Series("new2", [4, 5, 6])])
```


```{python}
df.clone() # deep copy
```

The following commands make changes in place; I am thus creating a copy of `df`.
```{python}
df_copy = df.clone() # making a copy since 
df_copy.insert_at_idx(1, pl.Series("new", [1, 2, 3])) 
```


```{python}
df_copy.replace_at_idx(0, pl.Series("new2", [1, 2, 3]))
```


```{python}
df_copy.replace('float', pl.Series("new_float", [4.0, 5.0, 6.0])) 
```


```{python}
def foo(frame):
  return frame.with_column(pl.Series("new", [1, 2, 3]))
df.pipe(foo)
```




```{python}
df.is_empty()
```


```{python}
df.cleared() # make empty copy
```

```{python}
df.cleared().is_empty()
```



Renaming columns can be done with `rename()`. 
Later, we will see it may also be done with an `alias()` statement withing a `with_columns()` context. 
```{python}
df.rename({'integer': 'integer2'})
```

## Convert to Other Python Objects
TODO

### To Pandas

### To Numpy

### To Python List

### To Python Dict

### To Python Tuple



## Dataframe in Memory

```{python}
df.estimated_size(unit="gb")
```


```{python}
df.n_chunks() # number of ChunkedArrays in the dataframe
```

```{python}
df.rechunk() # ensure contiguous memory layout
```

```{python}
df.shrink_to_fit() # reduce memory allocation to actual size
```





## Statistical Aggregations 

```{python}
df.describe()
```

Compare to pandas:
```{python}
df.to_pandas().describe()
```

Things to note:

- Polas will summarize all columns, even if they are not numeric.
- The statistics returned are different.
- In the following, Polars will always return a frame with the same number of columns as the original frame; pandas would have returned columns only where the operation is defined, and omit NAs. 

Statistical aggregations operate column-wise (and in parallel).
```{python}
df.max()
```


```{python}
df.min()
```

```{python}
df.mean()
```

```{python}
df.median()
```

```{python}
df.sum()
```

```{python}
df.std()
```

```{python}
df.quantile(0.1)
```




## Exctraction

1. If you are used to pandas, recall there is no index. There is thus no need for `loc` vs. `iloc`, `reset_index()`, etc.
2. Filtering and selection is possible with the `[` operator, or the `filter()` and `select()` methods. The latter is recommended to facilitate lazy evaluation (discussed later).



Single cell extraction.
```{python}
df[0,0] # like pandas .iloc[]
```

Slicing along rows.
```{python}
df[0:1] 
```

Slicing along columns.
```{python}
df[:,0:1]
```



### Filtering Rows

```{python}
df.head(2)
```


```{python}
df.limit(2) # same as pl.head()
```

```{python}
df.tail(1)
```

```{python}
df.take_every(2)
```

```{python}
df.slice(offset=1, length=1)
```


```{python}
df.sample(1)
```

```{python}
df.row(1) # get row as tuple
```

```{python}
df.rows() # all rows as list of tuples
```

Row filtering by label
```{python}
df.filter(pl.col("integer") == 2)
```

Things to note:

- The `[` operator does not support indexing with boolean such as `df[df["integer"] == 2]`.
- The `filter()` method is recommended over `[` by the authors of polars, to facilitate lazy evaluation (discussed later).






### Selecting Columns

Column selection by label
```{python}
df.select("integer")
# or df['integer']
# or df[:,'integer']
```


Multiple column selection by label
```{python}
df.select(["integer", "float"])
# or df[['integer', 'float']]
```

Column slicing by label
```{python}
df[:,"integer":"float"]
```
Note: Slicing with `df.select()` does not support ranges such as `df.select("integer":"float")`; only lists of column names.

Get a column as a 1D polars frame.
```{python}
df.get_column('integer')
```

Get a column as a polars series.
```{python}
df.to_series(0)
```

```{python}
df.find_idx_by_name('float')
```

```{python}
df.get_columns() # get a list of series
```



```{python}
df.drop("integer")
```

Polars will not have an `inplace` argument. Use `df.drop_in_place()` instead.


Select along dtype
```{python}
df.select(pl.col(pl.Int64))
```


```{python}
df.select(pl.col(pl.Float64))
```


```{python}
df.select(pl.col(pl.Utf8))
```


Things to note:

- The `pl.col()` function will be very useful for referencing columns in a dataframe. It may extract a single column, a list, a particular (polars) dtype, a regex pattern, or simply all columns.
- When exctracting along dtype, use polars' dtypes, not pandas' dtypes. For example, use `pl.Int64` instead of `np.int64`.




### Selecting A Single Item

Exctracts the first element as a scalar. Useful when you output a single number as a frame object. 
```{python}
pl.DataFrame([1]).item() # notice the output is not a frame, rather, a scalar.
```









## Uniques and Duplicates


```{python}
df.is_unique()
```

```{python}
df.is_duplicated()
```

```{python}
df.unique() # same as pd.drop_duplicates()
```



```{python}
df.n_unique()
```


## Missing

```{python}
df_with_nulls = df.with_columns([
    pl.Series("missing", [3, None, np.nan]),
])
```


```{python}
df_with_nulls.null_count() # same as pd.isnull().sum()
```

```{python}
df_with_nulls.drop_nulls() # same as pd.dropna()
```

```{python}
df_with_nulls.fill_null(0) # same as pd.fillna(0)
```

But recall that `None` and `np.nan` are not the same thing. 
```{python}
df_with_nulls.fill_nan(99)
```

```{python}
df_with_nulls.interpolate()
```









## Transformations

- The general idea of colum trasformation is to wrap all transformations in a `with_columns()` method, and the select colums to operat on with `pl.col()`. 
- The output column will have the same name as the input, unless you use the `alias()` method to rename it. 
- The `with_columns()` is called a __polars context__.
- The flavor of the `with_columns()` context is similar to pandas' `assign()`.
- One can use `df.iter_rows()` to get an iterator over rows. 



```{python}
df.with_columns([
    pl.col("integer").alias("integer2"),
    pl.col("integer") * 2
])
```

Things to note:

- The column `integer` is copied, by renaming it to `integer2`.
- The columns `integer` is multiplied by 2 in place, because no `alias` is used. 
- You cannot use `[` to assign! This would not have worked `df['integer3'] = df['integer'] * 2`




If a selection returns multiple columns, all will be transformed:
```{python}
df.with_columns([
    pl.col([pl.Int64,pl.Float64])*2
])
```

```{python}
df.with_columns([
    pl.all()*2
])
```

Apply your own labda function. 
```{python}
df.select([pl.col("integer"), pl.col("float")]).apply(lambda x: x[0] + x[1])
```

But wait- using your own functions may have a very serious toll on perforance:
```{python}
df_big = pl.DataFrame(np.random.randn(1000000, 2), columns=["a", "b"])
%timeit -n2 -r2 df_big.sum(axis=1)
```

```{python}
%timeit -n2 -r2 df_big.apply(lambda x: x[0] + x[1])
```


```{python}
df.shift(1)
```

```{python}
df.shift_and_fill(1, 'WOW')
```


## Sorting

```{python}
df.sort("integer")
```

```{python}
df.reverse()
```









## Joins

High level: 

- `df.hstack()` for horizontal concatenation; like pandas `pd.concat([],axis=1)` or R's cbind.
- `df.vstack()` for vertical concatenation; like pandas `pd.concat([],axis=0)` or R's rbind.
- `df.join()` for joins; like pandas `pd.merge()` or `df.join()`.


```{python}
new_column = pl.Series("c", np.repeat(1, df.height))

df.hstack([new_column])
```

```{python}
df2 = pl.DataFrame({
  "integer": [1, 2, 3], 
  "date": [
    (datetime(2022, 1, 4)), 
    (datetime(2022, 1, 5)), 
    (datetime(2022, 1, 6))], 
    "float":[7.0, 8.0, 9.0],
    "string": ["d", "d", "d"]})


df.vstack(df2)
```

```{python}
df.extend(df2) # like vstack, but with memory re-chunking. Similar to df.vstack().rechunk()
```

```{python}
df.merge_sorted(df2, key="integer") # vstacking with sorting (aka riffle-shuffle).
```


__Caution__: Joining along rows is possible only if matched columns have the same dtype. 
Timestamps may be tricky because they may have different time units.
Recall that timeunits may be cast before joining using `series.dt.cast_time_unit()`:
```{python}
#| eval: false
df.with_column(
    pl.col(pl.Datetime("ns")).dt.cast_time_unit(tu="ms")
)            
```


```{python}
df.join(df2, on="integer", how="left")
```

Things to note:

- Repeating column names have been suffixed with "_right".
- Unlike pandas, there are no indices. The `on`/`left_on`/`right_on` argument is always required.
- `how=` may take the following values: 'inner', 'left', 'outer', 'semi', 'anti', 'cross'.
- The join is super fast, as demonstrated in the Motivation section above. 




```{python}
df.join_asof(df2, left_on="date", right_on='date2', by="integer", strategy="backward", tolerance='1w')
```

Things to note:

- Yes! `merge_asof()` is also available.
- The `strategy=` argument may take the following values: 'backward', 'forward'.
- The `tolerance=` argument may take the following values: '1w', '1d', '1h', '1m', '1s', '1ms', '1us', '1ns'.









## Reshaping

```{python}
df.transpose()
```

### Wide to Long



```{python}
# The following example is adapted from Pandas documentation: https://pandas.pydata.org/docs/reference/api/pandas.wide_to_long.html

np.random.seed(123)
wide = pl.DataFrame({
    'famid': ["11", "12", "13", "2", "2", "2", "3", "3", "3"],
    'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],
    'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],
    'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]})

wide.head(2)
```

```{python}
wide.melt(
  id_vars=['famid', 'birth'], 
  value_vars=['ht1', 'ht2'], 
  variable_name='treatment', 
  value_name='height').sample(5)
```

Break strings into rows. 
```{python}
wide.explode(columns=['famid']).limit(5)
```



### Long to Wide

```{python}
# Example adapted from https://stackoverflow.com/questions/5890584/how-to-reshape-data-from-long-to-wide-format

long = pl.DataFrame({
    'id': [1, 1, 1, 2, 2, 2, 3, 3, 3],
    'treatment': ['A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'B'],
    'height': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1]
    })
  
long.limit(5)
```       

```{python}
long.pivot(
  index='id', 
  columns='treatment',
  values='height')
```


```{python}
long.unstack(step=2) # works like a transpose, and then wrap rows. Change the `step=` to get the feeling. 
```



## Groupby

```{python}
df2 = pl.DataFrame({
    "integer": [1, 1, 2, 2, 3, 3],
    "float": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
    "string": ["a", "b", "c", "d", "e", "f"],
    "datetime": [
        (datetime(2022, 1, 4)), 
        (datetime(2022, 1, 4)), 
        (datetime(2022, 1, 4)), 
        (datetime(2022, 1, 9)), 
        (datetime(2022, 1, 9)), 
        (datetime(2022, 1, 9))],
})
```


```{python}
df2.partition_by("integer")
```


```{python}
groupper = df2.groupby("integer")
groupper.count()
```

```{python}
groupper.sum()
```

Groupby a fixed time window with `df.groupby_dynamic()`:
```{python}
(
  df2
  .groupby_dynamic(index_column="datetime", every="1d")
  .agg(pl.col("float").sum())
)
```

If you do not want a single summary per period, rather, a window at each datapoint, use `df.groupby_rolling()`:
```{python}
(
  df2
  .groupby_rolling(index_column="datetime", period='1d')
  .agg(pl.col("float").sum())
)
```







# Query Planning and Optimization

The take-home of this section, is that polar can take advantage of half-a-century's worth of research in query planning and optimization.
You will not have to think about the right order of operations, or the right data structures to use. 
Rather, replace the polars dataframe with a polars lazy-dataframe, state all the operations you want, and just finish with a `collect()`.
Polars will take care of the rest, and provide you with the tools to understand its plan. 

We will not go into the details of the difference between a lazy and a non-lazy dataframe.
Just assume a lazy frame allows everything a non-lazy frame can do, but it does not execute the operations until you call `collect()`.
This is not entirely true, but you will get an informative error if you try to do something that is not supported.


Get your lazy dataframe:
```{python}
df_lazy = df.lazy()
```

State all your operations:
```{python}
q = (
  df_lazy
  .filter(pl.col("float") > 2.0)
  .filter(pl.col("float") > 3.0)
  .filter(pl.col("float") > 7.0)
  .select(["integer"])
  .sort("integer")
)
```

And now visualize the query. 
```{python}
q # same as q.show_graph(optimized=False)
```

```{python}
q.show_graph(optimized=True)
```


Things to note:

- You will need Graphviz installed to visualize the query plan.
- To understand the plan, you need some terminology from [relational databases](https://www.ibm.com/docs/en/informix-servers/14.10?topic=concepts-selection-projection). Namely:
  -  A _selection_ is a subset of rows, marked in the graph with a $$\sigma$$. 
  - A _projection_ is a subset of columns, marked in the graph with a $$\pi$$.
- The optimized plan removes redudancies, and orders the operations in the most efficient way.


You can now execute the plan with a `collect()`:
```{python}
q.collect()
```

```{python}
q.describe_plan()
```

For early stopping you can replace `collect()` with `fetch()`: 
```{python}
q.fetch(2)
```




# I/O

You will find that polars is blazing fast at reading and writing data.
This is due to:

1. Very good heuristics/rules implemented in the `read_csv` function.
1. The use of [Apache Arrow](https://arrow.apache.org/) as an internal data structure, which maps seamlesly to the parquet file format.
1. Parallelism, whenever possible. 
1. Lazy scans/imports, which allows the materialization only of required data; i.e., filters and projections are executed at scan time.


## Import

### From a Single File

Let's firs make a csv to import:
```{python}
df.write_csv("df.csv")
```

Import the csv into a non-lazy frame:
```{python}
pl.read_csv("df.csv")
```

Importing as a lazy frame:
```{python}
df_lazy = pl.scan_csv("df.csv")
```

Things become interesting when you manipulate the lazy frame before materializing it:
```{python}
q = (
  df_lazy
  .filter(pl.col("float") > 2.0)
  .filter(pl.col("float") > 3.0)
  .filter(pl.col("float") > 7.0)
  .select(["integer"])
  .sort("integer")
)

q.show_graph(optimized=True)
```

```{python}
q.collect()
```


Things to note:

- From the graph we see that the filtering ($$\sigma$$) is done at scan time, and not after the materialization of the data.
- To get the actual data, we naturally need to `collect()`.


Cleary, .csv is not the only format that can be read. It is possibly the least recommended. 
Other file types can be found [here](https://pola-rs.github.io/polars/py-polars/html/reference/io.html) and include: 
- Excel. 
- Arrow IPC: A binary format for storing columnar data. 
- Feather (V2): A portable columnar file format that is optimized for storing data in a fast and efficient manner, utilizing the Arrow IPC format internally. 
- Parquet (non-partitioned): A tabular file format (not columnar) that is optimized for long-term storage, more compressed than Feather. 
- JSON: Short for JavaScript Object Notation, a textual data-interchange format.
- Avro: A binary row-based format. 

Each of the above formats has a non-lazy reader using `pl.read_*` and a lazy reader using `pl.scan_*`.

Not suported formats:
- Feather (V1). 
- HDF5: Currently [not supported](https://github.com/pola-rs/polars/issues/3520).



### From Multiple Files

- Parquet (partitioned): A collection of files with a common schema, partitioned as folders on disk.
- Delta Lake: Extends the Parquet file format. 
- Arrow Dataset: A collection of files (csv, parquet, feather, etc) with a common schema.


#### Partitioned Parquet



#### Apache Arrow Dataset

An Apache Arrow dataset is a collection of parquet files, with a common schema.
It is a very efficient way to store data on disk, and to read it in parallel.

Writing an Arrow dataset:

```{python}
df.to_pandas().to_parquet("df", engine="pyarrow", partition_cols=["integer"])

os.listdir("df")
```

```{python}
[os.listdir(f"df/{x}/") for x in os.listdir("df")]
```

```{python}
import pyarrow.dataset as ds
dset = ds.dataset("df", format="parquet")  
pl.scan_ds(dset).collect()
```


Things to note:

- We used pandas to write the arrow dataset. 
- The `partition_cols` argument is used to partition the dataset on disk. Each partition is a parquet file (or another partition).
- Reading from the web (not from the local filesystem) is slightly different. TODO: add reference. 


__Caution__:

The arrow data format uses caching for string and categorical data (i.e. pl.Series). 
If importing multiple files, such as multiple parquet/feather files, or an arrow dataset, different files may be cached differnetly. This will cause an error when trying to concatenate the dataframes.
To avoid this, you can disable string caching, or enforce [joint caching](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.StringCache.html) of all files. 
The latter will look like this:

```{python}
with pl.StringCache():
  df_import = pl.scan_ds(dset).collect()
df_import.head(3)
```


#### Multiple CSVs

TODO: `pl.read_csv_batched()`



### Reading from a Database
TODO: SQL, Delta Lake.



## Export

TODO








# Plotting

To get an intuition of what you may expect in this chapter you should know the following.
There are approaches to plotting in python:

1. The object oriented, where a dataframe has a plotting method. The method may use a single, or even multiple backends. Such is the pandas dataframe, which may use a matplotlib, plotly, or bokeh backend.
2. The functional method, where a plotting function takes a dataframe as an argument. Such are the matplotlib, seaborn, and plotly functions, which may take pandas dataframes as inputs. 

Plotting support in polars thus boils down to the folowing questions:
(1) Do polars dataframes have a plotting method? With which backend?
(2) Can plotting functions take polars dataframes as inputs?

The answer to the first is negative. 
Polars dataframes do not have a plotting method, and it seems they are not planned to have one (TODO: add reference).
The answer to the second is "almost yes" 
Any plotting function that can take numpy 1D arrays as inputs, can take a polars series after a zero copy conversion with `pl.Series.to_numpy()`.
Some plotting functions will not even require the conversion to numpy (and will handle it internally).

Polars frames may cause trouble. You may expect to use a `plot(df, x='col1', y='col2')` syntax; it may work if `df` is a pandas dataframe, but not with polars.
Support of this syntax does not depend on polars developers, rather, on the plotting function developpers. 
I suspect that the plotly and bokeh teams will eventually supprts polars. I do not know about the seaborm, or matplotlib teams.



## Plotly Functions

The `iris` dataset is provided by plotly as a pandas frame. 
We convert it to a polars frame.
```{python}
iris = pl.DataFrame(px.data.iris())
iris.head()
```

Plotly's `scatter()` can take x and y as str, int or Series or array-like. The following will naturally work:
```{python}
fig = px.scatter(
    x=iris["sepal_width"].to_list(), 
    y=iris["sepal_length"].to_list())
fig.show()
```

But wait! Maybe a polars series is "array-like" and can be used as input? 
Yes it can!

```{python}
fig = px.scatter(
    x=iris["sepal_width"], 
    y=iris["sepal_length"])
fig.show()
```

Can a polars frame be used as input?
No it can not. The following will not work:
```{python}
#| eval: false
#| 
fig = px.scatter(data_frame=iris,
    x="sepal_width",
    y="sepal_length")
fig.show()
```



## Matplotlib Functions

The above discussion applies to matplotlib functions as well; with the exception that matplotlib functions already support polars frames as input.

Inputing polars series:
```{python}
fig, ax = plt.subplots()
ax.scatter(
    x=iris["sepal_width"], 
    y=iris["sepal_length"])
```


Inputing polars frames:
```{python}
fig, ax = plt.subplots()
ax.scatter(
    data=iris,
    x="sepal_width",
    y="sepal_length")
```




# Polars and ML

"How do to machine learning with polars?" is not a well defined question. 
ML can be done with many libraries, and the answer depends on the library you are using.
One possibility is converting polars dataframes to a numpy arrays. This is very easy with `pl.Float64` and `pl.Int64` series which convert trivially. 
Converting `pl.Utf8` and `pl.Categorical` dtypes is a bit more involved, but still possible. For instance, by using `polars.DataFrame.to_dummies()`, `polars.get_dummies()`, or `polars.Series.to_dummies()`.


## Polars and Patsy

Patsy is a python library for describing statistical models (especially linear models and generalized linear models) and building design matrices.

```{python}
import patsy as pt

#make a dataframe
data = pd.DataFrame(
    np.random.randn(100, 3), 
    columns=["y", "x1", "x2"])
```

Use patsy to make a design matrix $$X$$, and a target vector $$y$$ from a pandas dataframe.
```{python}
formula = 'y ~ x1 + x2'
y, X = pt.dmatrices(formula, data)

X[:3]
```

```{python}
y[:3]
```

Does the same work with polars? Yes!
```{python}
data_polars= pl.DataFrame(data)
X, y = pt.dmatrices(formula, data_polars)
X[:3]
```





## Effect Coding and Contrasts

There are [many ways](https://stats.oarc.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis/) to encode categorical variables.
For predictions, dummy coding is enough. 
If you want to discuss and infer on effect sizes, you may want to use other coding schemes.

One way to go about is to use the [category_encoders](http://contrib.scikit-learn.org/category_encoders/) library.

We start by making some categorical data.
```{python}
import string
import random
cat = pl.Series(
    name="cat",
    values=random.choices(
        population=string.ascii_letters[:5], 
        k=data_polars.height)
    ).to_frame()
data_polars = data_polars.hstack(cat)
data_polars.head()
```

The category encoders currently expects pandas dataframes as input, and does not support polars dataframes.
```{python}
import category_encoders as ce
encoder = ce.HelmertEncoder()
encoder.fit(data_polars.to_pandas())
```


			



# Config

```{python}
list(dir(pl.Config))
```

